{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Conformers (https://arxiv.org/abs/2005.08100) are a powerful neural network architecture that takes advantage of both the local reasoning of convolutional networks with the global connections that transformers are able to provide. They have shown state-of-the-art results in automated speech recognition (ASR)\n",
        "\n",
        "In this notebook, we will use components that you've learned about in previous parts as well as some new ones to implement the convolution module of a Conformer, and then combine that with the other modules to form the entire Conformer block as described in the paper.\n",
        "\n",
        "Then, in the next notebook, we will explore one powerful application of Conformer models: speech recognition, where local relations and global context are both key to transcribing audio into text."
      ],
      "metadata": {
        "id": "WWYPIIC3HKcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup libraries and helpers\n",
        "!pip install einops\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def _set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDidYSayVf1r",
        "outputId": "839eefc8-6277-4dbb-f9a2-bf569b654429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Convolution Module\n",
        "\n",
        "We start by implementing our Convolution module, which contains several components that you had some practice with earlier.\n",
        "\n",
        "For your reference, this is the illustration of the module provided by the paper:\n",
        "\n",
        "![conv module](https://drive.google.com/uc?export=view&id=1aHxwDy0RjBEhNPeRzWwoQMrAUG4NZizA)\n",
        "\n",
        "First, fill in the blanks to complete the pointwise and depthwise convolution modules.\n",
        "\n",
        "HINT: For speech data, what would each of the dimensions represent? Therefore, would it make sense to use `nn.Conv1d` or `nn.Conv2d`?"
      ],
      "metadata": {
        "id": "mLG6yGXYUaOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointwiseConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels: int, \n",
        "        out_channels: int, \n",
        "        stride=1, \n",
        "        padding=0, \n",
        "        bias=True\n",
        "    ):\n",
        "        super(PointwiseConv, self).__init__()\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.conv = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "        return None\n",
        "\n",
        "class DepthwiseConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels: int, \n",
        "        out_channels: int,\n",
        "        kernel_size: int, \n",
        "        stride=1, \n",
        "        padding=0, \n",
        "        bias=False\n",
        "    ):\n",
        "        super(DepthwiseConv, self).__init__()\n",
        "        ### YOUR CODE HERE ###\n",
        "        self.conv = None \n",
        "\n",
        "    def forward(self, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "BS8Iaa0kKtvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "_set_seed(2023)\n",
        "p_model = PointwiseConv(2, 3)\n",
        "x = torch.tensor([[[1., 2.], [3., 4.]], \n",
        "                  [[5., 6.], [7., 8.]]])\n",
        "output = p_model(x)\n",
        "assert output.shape == torch.Size([2, 3, 2])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor(\n",
        "        [\n",
        "            [[0.7040, 0.9148],\n",
        "            [0.1491, 0.7541],\n",
        "            [1.6921, 2.4444]],\n",
        "\n",
        "            [[1.5473, 1.7581],\n",
        "            [2.5691, 3.1741],\n",
        "            [4.7013, 5.4536]]\n",
        "        ],\n",
        "        dtype=torch.float\n",
        "    ),\n",
        "    rtol=1e-03\n",
        ")\n",
        "\n",
        "d_model = DepthwiseConv(2, 4, 2)\n",
        "output = d_model(x)\n",
        "assert output.shape == torch.Size([2, 4, 1])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor([\n",
        "            [[ 0.4918], [-0.1206], [-0.3651], [-1.3516]],\n",
        "            [[ 0.8677], [-1.4797], [-0.2528], [-2.6693]]\n",
        "        ],\n",
        "        dtype=torch.float\n",
        "    ),\n",
        "    rtol=1e-03\n",
        ")\n"
      ],
      "metadata": {
        "id": "mAmLqgDzw3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement our activation functions. Recall that the swish function is\n",
        "$$y=\\frac{x}{1+e^{-x}}$$\n",
        "\n",
        "GLU stands for Gated Linear Unit, and was first introduced in [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083). \n",
        "\n",
        "The idea is similar to self-attention, where we use one view of the data (query vs gate) in order to weigh another view of that data (value vs output). In this case, instead of taking the product of the query and key, GLU takes the sigmoid in order to generate \"gates\", values between 0 and 1, that determine how much of each output passes through.\n",
        "\n",
        "$$h_l(\\mathbf X)=(\\mathbf X*\\mathbf W+\\mathbf b)\\otimes \\sigma(\\mathbf X*\\mathbf V+\\mathbf c)$$\n",
        "\n",
        "($\\otimes$ is the element-wise product)\n",
        "\n",
        "The function defined in the paper (as seen above) applies a linear transformation to the input $\\mathbf X$ to generate the output $\\mathbf X*\\mathbf W+\\mathbf b$ and the gate $\\mathbf X*\\mathbf V+\\mathbf c$. In our implementation, we will instead split our input into two equally-sized matrices to use as our output and gate (with the assumption that length along the split dimension is even)."
      ],
      "metadata": {
        "id": "TgtHPFaZY1G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "        return None\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super(GLU, self).__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### START OF YOUR CODE ###\n",
        "        outputs = None\n",
        "        gate = None\n",
        "        return None\n",
        "        ### END OF YOUR CODE ###"
      ],
      "metadata": {
        "id": "qEB2Gur_ZNVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "x = torch.tensor([3., 1., 4., 1., 5., 9., 2., 6.])\n",
        "\n",
        "swish = Swish()\n",
        "output = swish(x)\n",
        "assert output.shape == torch.Size([8])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor([2.8577, 0.7311, 3.9281, 0.7311, 4.9665, 8.9989, 1.7616, 5.9852]),\n",
        "    rtol=1e-04\n",
        ")\n",
        "\n",
        "glu = GLU(0)\n",
        "output = glu(x)\n",
        "assert output.shape == torch.Size([4])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor([2.9799, 0.9999, 3.5232, 0.9975]),\n",
        "    rtol=1e-04\n",
        ")"
      ],
      "metadata": {
        "id": "IZRzy7TIw9M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we connect them to form our Convolution and Feed Forward modules. \n",
        "\n",
        "The convolution module is already implemented for you, but you need to fill in the blanks to complete the feed forward module, which is depicted here.\n",
        "\n",
        "![ff module](https://drive.google.com/uc?export=view&id=1oAYXoUbKSh-Pp7-bIvu4rCpM4WqBXqz_)\n"
      ],
      "metadata": {
        "id": "oFRrRShvhme9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transpose(nn.Module):\n",
        "    \"\"\"Transposes the time and channel dimensions for use in nn.Sequential\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels: int, \n",
        "        kernel_size: int, \n",
        "        dropout: float\n",
        "    ):\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        assert (kernel_size - 1) % 2 == 0, \"kernel_size should be a odd number for 'SAME' padding\"\n",
        "        padding = (kernel_size-1) // 2\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.LayerNorm(in_channels),\n",
        "            Transpose(),\n",
        "            PointwiseConv(in_channels, in_channels * 2),\n",
        "            GLU(dim=1),\n",
        "            DepthwiseConv(in_channels, in_channels, kernel_size, padding=padding),\n",
        "            nn.BatchNorm1d(in_channels),\n",
        "            Swish(),\n",
        "            PointwiseConv(in_channels, in_channels),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sequential(x).transpose(1, 2)\n",
        "\n",
        "class FeedForwardModule(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        hidden_dim: int, \n",
        "        out_dim: int, \n",
        "        dropout: float\n",
        "    ):\n",
        "        super(FeedForwardModule, self).__init__()\n",
        "        self.sequential = nn.Sequential(\n",
        "            ### YOUR CODE HERE ###\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sequential(x)"
      ],
      "metadata": {
        "id": "eUo0JBdwiIPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "\n",
        "_set_seed(4)\n",
        "ff = FeedForwardModule(2, 3, 2, 0.2)\n",
        "x = torch.tensor([[9., 8.], [7., 6.], [5., 4.], [3., 2.]])\n",
        "output = ff(x)\n",
        "assert output.shape == torch.Size([4, 2])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor(\n",
        "        [[0.0000, 0.3803],\n",
        "        [1.7393, 0.0000],\n",
        "        [0.4592, 0.3812],\n",
        "        [0.5688, 0.3707]]\n",
        "    ),\n",
        "    rtol=1e-03\n",
        ")"
      ],
      "metadata": {
        "id": "vrkN9T8ew-hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Convolution Block\n",
        "\n",
        "Now we combine everything we've built to make our entire Convolution block!\n"
      ],
      "metadata": {
        "id": "nhonilIOhJ1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an implementation of the Multi-Head Self Attention module. You only need to run this block and use the module in your implementation.\n"
      ],
      "metadata": {
        "id": "LQ6UvvuXxT8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Attention Module\n",
        "\n",
        "# Source: https://github.com/lucidrains/conformer\n",
        "class AttentionModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        dropout = 0.,\n",
        "        max_pos_emb = 512\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads= heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.max_pos_emb = max_pos_emb\n",
        "        self.rel_pos_emb = nn.Embedding(2 * max_pos_emb + 1, dim_head)\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        n, device, h, max_pos_emb, has_context = x.shape[-2], x.device, self.heads, self.max_pos_emb, exists(context)\n",
        "        context = default(context, x)\n",
        "\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        # shaw's relative positional embedding\n",
        "        seq = torch.arange(n, device = device)\n",
        "        dist = rearrange(seq, 'i -> i ()') - rearrange(seq, 'j -> () j')\n",
        "        dist = dist.clamp(-max_pos_emb, max_pos_emb) + max_pos_emb\n",
        "        rel_pos_emb = self.rel_pos_emb(dist).to(q)\n",
        "        pos_attn = torch.einsum('b h n d, n r d -> b h n r', q, rel_pos_emb) * self.scale\n",
        "        dots = dots + pos_attn\n",
        "\n",
        "        if exists(mask) or exists(context_mask):\n",
        "            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))\n",
        "            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))\n",
        "            mask_value = -torch.finfo(dots.dtype).max\n",
        "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(context_mask, 'b j -> b () () j')\n",
        "            dots.masked_fill_(~mask, mask_value)\n",
        "\n",
        "        attn = dots.softmax(dim = -1)\n",
        "\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return self.dropout(out)"
      ],
      "metadata": {
        "id": "DaIlibftsZ7w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, implement the Conformer block. Remember to add residual connections between the modules.\n",
        "\n",
        "![conv block](https://drive.google.com/uc?export=view&id=1a4L1UcxjRdzIrPxnotdWS0zuUts4UHfy)"
      ],
      "metadata": {
        "id": "fTfQGEJ2teiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads,\n",
        "        dim_head,\n",
        "        ff_dim,\n",
        "        kernel_size,\n",
        "        attn_dropout,\n",
        "        ff_dropout,\n",
        "        conv_dropout\n",
        "    ):\n",
        "        super(ConformerBlock, self).__init__()\n",
        "\n",
        "        ### START OF YOUR CODE  ###\n",
        "        self.ff1 = None\n",
        "        self.attn = None\n",
        "        self.conv = None\n",
        "        self.ff2 = None\n",
        "        self.norm = None\n",
        "        ### END OF YOUR CODE ###\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        out = x\n",
        "        ### YOUR CODE HERE  ###\n",
        "        return out"
      ],
      "metadata": {
        "id": "IZjgvKTdhIlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "\n",
        "_set_seed(420)\n",
        "model = ConformerBlock(3, 4, 4, 4, 3, 0.2, 0.3, 0.4)\n",
        "x = torch.tensor([[[1., 2., 3.], [4., 5., 6.]], \n",
        "                  [[-1., 0., 1.], [9., 8., 7.]]])\n",
        "output = model(x)\n",
        "assert output.shape == torch.Size([2, 2, 3])\n",
        "assert torch.allclose(\n",
        "    output,\n",
        "    torch.tensor(\n",
        "        [[[ 1.0219, -1.3575,  0.3357],\n",
        "         [-1.3136,  0.2031,  1.1105]],\n",
        "        [[-0.8080, -0.6011,  1.4092],\n",
        "         [ 1.3563, -0.3312, -1.0251]]]\n",
        "    ),\n",
        "    rtol=1e-03\n",
        ")"
      ],
      "metadata": {
        "id": "Czi1QHM-xAiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}